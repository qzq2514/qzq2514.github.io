<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>Georgeqi's Blog</title><meta name="description" content="The harder you work, the luckier you will be~"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/widget-post-list.css"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Georgeqi_Blog" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">Georgeqi's Blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><span>Archives · All</span></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><article class="post-container is-flex is-justify-content-center section container is-max-widescreen pt-4 px-2"><div class="columns is-variable is-1-tablet is-3-desktop-only is-2-widescreen is-full-width"><section class="column"><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2023/10/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E6%96%B0%E8%A7%86%E8%A7%92%E7%94%9F%E6%88%90(NVS)/NVS.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2023/10/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E6%96%B0%E8%A7%86%E8%A7%92%E7%94%9F%E6%88%90(NVS)/"><img class="post-cover-img js-img-fadeIn" src="/2023/10/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E6%96%B0%E8%A7%86%E8%A7%92%E7%94%9F%E6%88%90(NVS)/NVS.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0"><i class="tag post-item-tag">论文笔记</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2023/10/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E6%96%B0%E8%A7%86%E8%A7%92%E7%94%9F%E6%88%90(NVS)/">论文笔记-新视角生成(NVS)</a></h2><time class="has-text-grey" datetime="2023-10-10T15:21:22.000Z">2023-10-10</time><p class="is-flex-grow-2 mt-2">新视角生成是从一个指定的图像或者prompt作为输入，生成该物体/场景在其他视角下的情况，要求生成的结果和原始的图像/Prompt是具有语义和纹理一致性的，同时如果生成多个视角还要保证多个视角之间也是一致的。下面Zero
1-to-3的效果就能很好表明这个问题：


新视角生成示意图(原图来自Zero 1-to-3)

【1】Zero-1-to-3:
Zero-shot One Image to 3D Object
【项目主页】【论文】【代码】


Zero123


算法流程：该算法借助当前强大的2D
下的Diffusion模型生成3D物体，具体步骤为:
该模型根据指定的输入图和相机变换参数，来生成该物体新位置下的新视角图，然后使用多张新视角图再配合Score Jacobian
chain来优化得到最终的..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2023/10/10/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E6%96%B0%E8%A7%86%E8%A7%92%E7%94%9F%E6%88%90(NVS)/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2023/06/10/Diffusion%E5%AD%A6%E4%B9%A06-%E7%94%9F%E6%88%90%E5%8F%AF%E6%8E%A7%E6%80%A7/Diffusion可控性.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2023/06/10/Diffusion%E5%AD%A6%E4%B9%A06-%E7%94%9F%E6%88%90%E5%8F%AF%E6%8E%A7%E6%80%A7/"><img class="post-cover-img js-img-fadeIn" src="/2023/06/10/Diffusion%E5%AD%A6%E4%B9%A06-%E7%94%9F%E6%88%90%E5%8F%AF%E6%8E%A7%E6%80%A7/Diffusion可控性.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Diffusion"><i class="tag post-item-tag">Diffusion</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2023/06/10/Diffusion%E5%AD%A6%E4%B9%A06-%E7%94%9F%E6%88%90%E5%8F%AF%E6%8E%A7%E6%80%A7/">Diffusion学习6-生成可控性</a></h2><time class="has-text-grey" datetime="2023-06-10T12:10:22.000Z">2023-06-10</time><p class="is-flex-grow-2 mt-2">Diffusion相比较起前辈GAN，在生成质量上已经得到了大幅度的提升，之前的文章介绍过虽然通过prompt配合上classifier-free
guidance技术能够从一定程度达到控制生成内容的目的，但是如何更加精确控制diffuison结果，例如定制化生成指定的目标对象，控制生成目标的姿态、形状、颜色等，也成为后续研究的重点，当然这些控制性生成目前也已经能够达到下图所示的非常好的效果了，本篇文章也将重点介绍些这些控制diffusion生成的大杀器。


Diffusion可控性生成效果示意效果,原图来自huggingface/lllyasviel

DreamBooth
【主页】【论文】【官方仓库-无code】【Dreambooth-Stable-Diffusion】


DreamBooth


..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2023/06/10/Diffusion%E5%AD%A6%E4%B9%A06-%E7%94%9F%E6%88%90%E5%8F%AF%E6%8E%A7%E6%80%A7/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2023/05/20/Diffusion%E5%AD%A6%E4%B9%A05-%E9%AB%98%E8%B4%A8%E9%87%8F%E6%96%87%E7%94%9F%E5%9B%BE%E6%A8%A1%E5%9E%8B/Diffusion_show.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2023/05/20/Diffusion%E5%AD%A6%E4%B9%A05-%E9%AB%98%E8%B4%A8%E9%87%8F%E6%96%87%E7%94%9F%E5%9B%BE%E6%A8%A1%E5%9E%8B/"><img class="post-cover-img js-img-fadeIn" src="/2023/05/20/Diffusion%E5%AD%A6%E4%B9%A05-%E9%AB%98%E8%B4%A8%E9%87%8F%E6%96%87%E7%94%9F%E5%9B%BE%E6%A8%A1%E5%9E%8B/Diffusion_show.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Diffusion"><i class="tag post-item-tag">Diffusion</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2023/05/20/Diffusion%E5%AD%A6%E4%B9%A05-%E9%AB%98%E8%B4%A8%E9%87%8F%E6%96%87%E7%94%9F%E5%9B%BE%E6%A8%A1%E5%9E%8B/">Diffusion学习5-高质量文生图模型</a></h2><time class="has-text-grey" datetime="2023-05-20T12:10:22.000Z">2023-05-20</time><p class="is-flex-grow-2 mt-2">

高质量Diffusion模型生成结果(原图来自DALLE-2)

DALLE
【论文】【代码-非官方】
DALL·E的名字是为了向艺术家萨尔瓦多·达利和皮克斯的机器人WALL-E致敬
DALLE-2
GLIDE
【论文】【代码】


GLIDE利用文本进行图像编辑


算法流程：本篇论文并没有做过多的算法创新，更多的是验证性和实验工程相关的内容，其提出可以用文本当做条件去合成、控制和编辑图像。
算法细节：

比较了两种文本引导图像生成的方式:

Classifier-Free-Guidance:
虽然该技术介绍过很多次，但是为了保证相对独立性，这里还是把本篇文章下的CFG公式写一下:
 其中这里的专门代表文本条件。
CLIP Guidance:  其中分别表示从CLIP提取的图像特征和文本特征，两者相乘..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2023/05/20/Diffusion%E5%AD%A6%E4%B9%A05-%E9%AB%98%E8%B4%A8%E9%87%8F%E6%96%87%E7%94%9F%E5%9B%BE%E6%A8%A1%E5%9E%8B/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2023/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E5%AF%B9%E7%99%BD%E8%86%9CMesh%E4%B8%8A%E8%89%B2/intro.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2023/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E5%AF%B9%E7%99%BD%E8%86%9CMesh%E4%B8%8A%E8%89%B2/"><img class="post-cover-img js-img-fadeIn" src="/2023/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E5%AF%B9%E7%99%BD%E8%86%9CMesh%E4%B8%8A%E8%89%B2/intro.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Mesh%E7%BA%B9%E7%90%86%E7%94%9F%E6%88%90"><i class="tag post-item-tag">Mesh纹理生成</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2023/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E5%AF%B9%E7%99%BD%E8%86%9CMesh%E4%B8%8A%E8%89%B2/">论文笔记-对白膜Mesh上色</a></h2><time class="has-text-grey" datetime="2023-04-22T11:27:23.000Z">2023-04-22</time><p class="is-flex-grow-2 mt-2">如何对已有的3D白膜生成纹理(即进行上色)是计算机视觉领域一个重要的领域，主要效果就如下图所示:


3D Mesh上色示意图(原图来自论文TEXTure)

这项技术能够帮助我们对同一个mesh生成不同风格/色调的&quot;外衣&quot;，对于扩充mesh的丰富性和可用性具有重要意义，这里就简单介绍一下最近读的4篇mesh上色的工作。
【1】TEXTure:
Text-Guided Texturing of 3D Shapes
【项目主页】【论文】【代码】
先贴上论文中给出的算法流程吧:


TEXTure算法流程


算法流程:
该算法借助当前在2D图像领域大火的Diffusion技术，在视角空间对mesh前景生成纹理，然后通过可微分渲染的方式的方式将视角空间的纹理一点点&quot;贴&quot;回mesh
算法细节：

首先要知道的是:..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2023/04/22/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E5%AF%B9%E7%99%BD%E8%86%9CMesh%E4%B8%8A%E8%89%B2/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/ImprovedDDPM_figure3.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/"><img class="post-cover-img js-img-fadeIn" src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/ImprovedDDPM_figure3.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Diffusion"><i class="tag post-item-tag">Diffusion</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/">Diffusion学习4-效果和性能提升</a></h2><time class="has-text-grey" datetime="2022-12-31T14:10:22.000Z">2022-12-31</time><p class="is-flex-grow-2 mt-2">下述效果的具体代码见: qzq2514/Diffusion
效果提升
这里简单实验和讨论了在训练DDPM过程中使用的损失函数(L1或L2损失)，并且实验了Improved
DDPM 中提到的Cosine Beta Schedule带来的效果提升。
Cosine Beta Schedule
首先回顾下DDPM在前向过程中的一个重要公式-利用重采样技巧从直接得到，即Diffusion学习2-理论推导中的公式(3):
 而且在原DDPM论文中使用的线性Beta采样，即:
torch.linspace(start=0.0001, end=0.02, steps=100)
Improved
DDPM则进一步分析了这种Beta设计的缺点，取出论文中的图3和图5:







Linear和Cosine的Beta方式对..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2022/12/25/Diffusion%E5%AD%A6%E4%B9%A03-%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5/20221228191848_StyleGAN_face_size128_225_progress.jpg" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2022/12/25/Diffusion%E5%AD%A6%E4%B9%A03-%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5/"><img class="post-cover-img js-img-fadeIn" src="/2022/12/25/Diffusion%E5%AD%A6%E4%B9%A03-%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5/20221228191848_StyleGAN_face_size128_225_progress.jpg" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Diffusion"><i class="tag post-item-tag">Diffusion</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2022/12/25/Diffusion%E5%AD%A6%E4%B9%A03-%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5/">Diffusion学习3-代码实践</a></h2><time class="has-text-grey" datetime="2022-12-25T13:27:59.000Z">2022-12-25</time><p class="is-flex-grow-2 mt-2">先放一张在人脸数据集上训练好后去噪的可视化过程


DDPM可视化去噪

各数据集参数配置及效果
训练:
CUDA_VISIBLE_DEVICES=0 python train_solver.py --data_name &quot;Flower102&quot;
在config.yaml中各个数据集使用默认的Training
Setting，每个数据集特有的配置见config.yaml下的Train_Data.
生成效果
生成效果如下:









数据集
去噪过程可视化
最终去噪效果
插值




Mnist





Fashion_Mnist





Cifar10





Flower102





StyleGAN2人脸






上述训练数据集和已经训练好的模型放在这里.

去噪过程可视化中，如果在..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2022/12/25/Diffusion%E5%AD%A6%E4%B9%A03-%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2022/12/20/Diffusion%E5%AD%A6%E4%B9%A02-%E7%90%86%E8%AE%BA%E6%8E%A8%E5%AF%BC/各种生成模型总览.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2022/12/20/Diffusion%E5%AD%A6%E4%B9%A02-%E7%90%86%E8%AE%BA%E6%8E%A8%E5%AF%BC/"><img class="post-cover-img js-img-fadeIn" src="/2022/12/20/Diffusion%E5%AD%A6%E4%B9%A02-%E7%90%86%E8%AE%BA%E6%8E%A8%E5%AF%BC/各种生成模型总览.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Diffusion"><i class="tag post-item-tag">Diffusion</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2022/12/20/Diffusion%E5%AD%A6%E4%B9%A02-%E7%90%86%E8%AE%BA%E6%8E%A8%E5%AF%BC/">Diffusion学习2-理论推导</a></h2><time class="has-text-grey" datetime="2022-12-20T13:27:59.000Z">2022-12-20</time><p class="is-flex-grow-2 mt-2">概述


生成模型总览(原图来自lilianweng.blog)

Diffusion属于生成模型的一种，相比较于GAN等其他生成模型，Diffusion模型最大的不同之处就在于其latent
code是和原输入图相同尺寸的。Diffusion模型其实也可以看成是一个隐变量模型，并且与VAE,GAN的单隐变量不同，其可以看成存在多个隐变量(即加噪过程中的每个加噪结果都可以看成一个隐变量)。Diffusion模型总体包括前向加噪和逆向去噪两个过程:

前向过程-加噪扩散:对给定的真实图像不断添加高斯噪声，经过中间状态最终变成纯高斯噪声
逆向过程-去噪生成:从完全的纯噪声不断去噪，经过中间状态最终变成其对应的真实图像

上面两个过程示意图可以表示如下:


Diffusion的前向和逆向过程(原图来自Ho et..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2022/12/20/Diffusion%E5%AD%A6%E4%B9%A02-%E7%90%86%E8%AE%BA%E6%8E%A8%E5%AF%BC/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2022/12/10/Diffusion%E5%AD%A6%E4%B9%A01-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/VAE示意图.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2022/12/10/Diffusion%E5%AD%A6%E4%B9%A01-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/"><img class="post-cover-img js-img-fadeIn" src="/2022/12/10/Diffusion%E5%AD%A6%E4%B9%A01-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/VAE示意图.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Diffusion"><i class="tag post-item-tag">Diffusion</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2022/12/10/Diffusion%E5%AD%A6%E4%B9%A01-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/">Diffusion学习1-预备知识</a></h2><time class="has-text-grey" datetime="2022-12-10T13:27:59.000Z">2022-12-10</time><p class="is-flex-grow-2 mt-2">Diffusion模型打响了AIGC的第一枪，我之前一直是做GAN这一块的图像生成，但是奈何Diffuison模型效果好的离谱，也趁着空闲时间抱着学习的态度看看到底是他是靠着什么&quot;吊打&quot;GAN的，但是在推Diffusion公式的时候，牵扯到很多比较细碎的小知识点，有一些因为不常用也都忘记了，这里正好补充在这里，方便后续快速查询。
关于Diffusion相关的论文推导后面会单独开一个新的文章进行介绍，如果有时间和精力应该还会补充代码的实践~
马尔科夫链

总体思想:
过去的所有信息都被保存在了现在的状态中，只使用现在状态就能预测到之后的状态，换句话说就是某个时刻的状态转移概率只依赖于它的前一个状态。
公式化表达:
举例:

股市涨跌：当前股市的涨、跌、平的状态概率分别为[0.3,0.4,0.3]，且转移概率矩..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2022/12/10/Diffusion%E5%AD%A6%E4%B9%A01-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2021/05/12/%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB%E5%AE%8C%E6%95%B4%E6%B5%81%E7%A8%8B/车牌识别流程.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2021/05/12/%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB%E5%AE%8C%E6%95%B4%E6%B5%81%E7%A8%8B/"><img class="post-cover-img js-img-fadeIn" src="/2021/05/12/%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB%E5%AE%8C%E6%95%B4%E6%B5%81%E7%A8%8B/车牌识别流程.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB"><i class="tag post-item-tag">车牌识别</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2021/05/12/%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB%E5%AE%8C%E6%95%B4%E6%B5%81%E7%A8%8B/">车牌识别完整流程</a></h2><time class="has-text-grey" datetime="2021-05-12T05:24:59.000Z">2021-05-12</time><p class="is-flex-grow-2 mt-2">研究生期间主要且完整的一个项目是美国的车牌识别项目，在这个项目中也积累了一些经验并且发了一些专利，本篇博客则是整合了这些经验并构建了一个完整的车牌识别流程，有兴趣的也可以直接去看【论文】和【代码】。
要解决的问题
该项目顾名思义就是完成车牌号码识别的目的，但是这里要识别的车牌是美国的车牌号码，它们相比较&quot;蓝底白字&quot;国内车牌来说更加复杂，具体体现在“堆叠字符”、“长度不固定”、“排列无规律”，具体这些车牌的示意图可以见关于车牌识别的三个专利。本项目则能相对来说比较好地解决这些特殊的问题，在此之外本项目对于车牌识别中的“车牌倾斜”、“光照不均”、“模糊”等普遍存在的问题也都有比较好的鲁棒性。
解决方案
整个流程接受任意环境下的车牌图像(如倾斜、光照不均等)，然后按照下图所示的流程进行识别：


车牌识别流程

..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2021/05/12/%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB%E5%AE%8C%E6%95%B4%E6%B5%81%E7%A8%8B/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2021/03/10/%E5%85%B3%E4%BA%8E%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB%E7%9A%84%E4%B8%89%E4%B8%AA%E4%B8%93%E5%88%A9/post_show.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2021/03/10/%E5%85%B3%E4%BA%8E%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB%E7%9A%84%E4%B8%89%E4%B8%AA%E4%B8%93%E5%88%A9/"><img class="post-cover-img js-img-fadeIn" src="/2021/03/10/%E5%85%B3%E4%BA%8E%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB%E7%9A%84%E4%B8%89%E4%B8%AA%E4%B8%93%E5%88%A9/post_show.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB"><i class="tag post-item-tag">车牌识别</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2021/03/10/%E5%85%B3%E4%BA%8E%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB%E7%9A%84%E4%B8%89%E4%B8%AA%E4%B8%93%E5%88%A9/">关于车牌识别的三个专利</a></h2><time class="has-text-grey" datetime="2021-03-10T12:32:09.000Z">2021-03-10</time><p class="is-flex-grow-2 mt-2">我研究生期间的主要做的是一个商业性质的车牌识别项目，该三个专利也是该项目期间积累下来的一些实践经验，趁着毕业正好整理一下。


复杂车牌示意图

【1】一种非二值化和边缘检测的车牌字符图像分割方法

相关资源：训练和推理代码
专利原文
要解决的问题：解决复杂车牌(如存在字符黏连、光照不均)中的字符分割难题
算法输入：车牌的字符区域(最好是高度和字符高度接近的)
算法输出：每个车牌字符的包围框坐标
算法流程：使用一个指定宽度的滑动窗口对图像从左到右进行遍历，每次遍历使用一个二分类的网络判断滑动窗口内是否是属于车牌字符(通常为字母和数字)的概率，最后得到一个字符概率的分布曲线，最后通过非极大值抑制得到最终的字符区域。
训练数据：
训练二分类网络：(判别是否是字母/数字)使用的2类数据-------&amp;gt;配合交..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2021/03/10/%E5%85%B3%E4%BA%8E%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB%E7%9A%84%E4%B8%89%E4%B8%AA%E4%B8%93%E5%88%A9/">Read more</a></section></article><section class="paginator is-flex is-justify-content-flex-end is-flex-wrap-wrap mt-5"><span class="page-number current">1</span><a class="page-number" href="/archives/page/2/">2</a><a class="extend next" rel="next" href="/archives/page/2/"><i class="iconfont icon-next has-text-grey"></i></a></section></section><aside class="column is-hidden-mobile is-4-tablet is-3-widescreen"><div style="position: sticky; top: 50px;"><main class="aside-card-container archives-widget is-in-archive-page"><h3>Archives</h3><section><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">May 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/12/">December 2022</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">May 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">February 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">October 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a><span class="archive-list-count">1</span></li></ul></section></main></div></aside></div></article><script>$claudia.fadeInImage(null, $claudia.blurBackdropImg)

window.addEventListener('resize', $claudia.throttle(function () {
    var images = document.querySelectorAll('.js-img-fadeIn')

    images.forEach($claudia.blurBackdropImg)
}, 150))</script></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com/qzq2514"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/qzq2514"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com/qzq2514"><i class="iconfont icon-ins"></i></a><!-- publish--><a title="publish" target="_blank" rel="noopener nofollow" href="https://blog.csdn.net/qzq2514"><i class="iconfont icon-rss"></i></a><!-- RSS--><!-- 知乎--><a title="zhihu" target="_blank" rel="noopener nofollow" href="//zhihu.com/people/qi-zhong-qi-62"><i class="iconfont icon-zhihu"></i></a><!-- 领英--><!-- 脸书--></section><p><span>Copyright ©</span><span> Georgeqi 2024</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script>$claudia.fadeInImage(null, $claudia.blurBackdropImg)

window.addEventListener('resize', $claudia.throttle(function () {
    var images = document.querySelectorAll('.js-img-fadeIn')

    images.forEach($claudia.blurBackdropImg)
}, 150))</script></body></html>
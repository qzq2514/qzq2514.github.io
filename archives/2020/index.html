<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>Georgeqi's Blog</title><meta name="description" content="The harder you work, the luckier you will be~"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/widget-post-list.css"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Georgeqi_Blog" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">Georgeqi's Blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><span>Archives · 2020</span></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><article class="post-container is-flex is-justify-content-center section container is-max-widescreen pt-4 px-2"><div class="columns is-variable is-1-tablet is-3-desktop-only is-2-widescreen is-full-width"><section class="column"><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2020/10/20/GAN%E5%AD%A6%E4%B9%A03-StyleGAN/StyleGAN生成人脸示意图.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2020/10/20/GAN%E5%AD%A6%E4%B9%A03-StyleGAN/"><img class="post-cover-img js-img-fadeIn" src="/2020/10/20/GAN%E5%AD%A6%E4%B9%A03-StyleGAN/StyleGAN生成人脸示意图.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/GAN"><i class="tag post-item-tag">GAN</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2020/10/20/GAN%E5%AD%A6%E4%B9%A03-StyleGAN/">GAN学习3-StyleGAN</a></h2><time class="has-text-grey" datetime="2020-10-19T16:00:00.000Z">2020-10-20</time><p class="is-flex-grow-2 mt-2">StyleGAN
【论文】【代码】【视频】【数据集】
先放一张StyleGAN生成人脸的效果图吧，确实真实到无法分辨出他是生成的，效果好到炸裂，以至于学习GAN不熟悉StyleGAN可以说说白学GAN了。


StyleGAN生成人脸示意图,来自video

当然除了人脸，StyleGAN在其他物体上的生成效果也是非常炸裂的。


StyleGAN生成其他物体示意图,来自video

写在前面
StyleGAN对于GAN的发展和推进太重要了，论文的内容很丰富细节也很多，一直没有很好的写作思路，直到看到了这篇博客和他的两篇中文版本:Wooo.zhihu和Yuthon's
Blog，发现大家写的太好了，于是近乎全文copy过来当做笔记也方便后面自己查阅，只是加了少量自己的图和理解，也建议
大家去原博客进行阅读。..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2020/10/20/GAN%E5%AD%A6%E4%B9%A03-StyleGAN/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2020/04/10/GAN%E5%AD%A6%E4%B9%A02-%E5%9B%BE%E5%83%8F%E8%BD%AC%E6%8D%A2/图像转换示意图.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2020/04/10/GAN%E5%AD%A6%E4%B9%A02-%E5%9B%BE%E5%83%8F%E8%BD%AC%E6%8D%A2/"><img class="post-cover-img js-img-fadeIn" src="/2020/04/10/GAN%E5%AD%A6%E4%B9%A02-%E5%9B%BE%E5%83%8F%E8%BD%AC%E6%8D%A2/图像转换示意图.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/GAN"><i class="tag post-item-tag">GAN</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2020/04/10/GAN%E5%AD%A6%E4%B9%A02-%E5%9B%BE%E5%83%8F%E8%BD%AC%E6%8D%A2/">GAN学习2-图像转换</a></h2><time class="has-text-grey" datetime="2020-04-10T02:32:59.000Z">2020-04-10</time><p class="is-flex-grow-2 mt-2">图像转换是GAN的一个重要应用，不同于原始GAN结构从一个维度较低(如512x1)的噪声生成图像，图像转换则接受图像级别(如256x256x3)的输入，并在保证云输入图像主要特征(如结构、人脸id)的情况下输出具有其他风格的相同尺寸图像。


图像转换示意图，原图来自CycleGAN

DTN
【Domain Transfer
Network 】【官方代码】【复现代码】
算法笔记


DTN


目标任务：将源于的图像S转换到目标域T中，并且在保留源域主要特征(如人脸id)的同时也具有目标域的特征(如卡通特征)。
算法细节:

网络结构：生成器G完成image2image任务，其由特征提取器f和重构器g构成；判别器D为三类判别，判别输入图像是否来自1.目标域真实图像；2.目标域重构生成的图像；3.源域转化生..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2020/04/10/GAN%E5%AD%A6%E4%B9%A02-%E5%9B%BE%E5%83%8F%E8%BD%AC%E6%8D%A2/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2020/02/27/%E8%82%A1%E7%A5%A8%E5%88%A9%E7%9B%8A%E6%9C%80%E5%A4%A7%E5%8C%96/cover_tree.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2020/02/27/%E8%82%A1%E7%A5%A8%E5%88%A9%E7%9B%8A%E6%9C%80%E5%A4%A7%E5%8C%96/"><img class="post-cover-img js-img-fadeIn" src="/2020/02/27/%E8%82%A1%E7%A5%A8%E5%88%A9%E7%9B%8A%E6%9C%80%E5%A4%A7%E5%8C%96/cover_tree.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/%E7%AE%97%E6%B3%95%E5%88%B7%E9%A2%98"><i class="tag post-item-tag">算法刷题</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2020/02/27/%E8%82%A1%E7%A5%A8%E5%88%A9%E7%9B%8A%E6%9C%80%E5%A4%A7%E5%8C%96/">股票利益最大化</a></h2><time class="has-text-grey" datetime="2020-02-27T02:45:26.000Z">2020-02-27</time><p class="is-flex-grow-2 mt-2">
问题定义
给定股票价格序列，规定(买入、卖出)的最多次数K，求最大的总利润。(一次买入和卖出为完整交易)
K=1
Leetcode121记录遍历到第i天的时候之前的股票价格最小值，那么如果当天卖出，能取得最大利润肯定是当前股票价格减去之前最低的股票价格最后取遍历的最大值。
class Solution {
public:
    int maxProfit(vector&amp;lt;int&amp;gt;&amp;amp; prices) {
        int max_profit = 0;
        int pre_min = INT_MAX;
        for(int i=0;i&amp;lt;prices.size();i++)
        {
            max_profit=max(max_..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2020/02/27/%E8%82%A1%E7%A5%A8%E5%88%A9%E7%9B%8A%E6%9C%80%E5%A4%A7%E5%8C%96/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2020/02/26/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%9D%E8%80%83/神经网络结构.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2020/02/26/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%9D%E8%80%83/"><img class="post-cover-img js-img-fadeIn" src="/2020/02/26/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%9D%E8%80%83/神经网络结构.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><i class="tag post-item-tag">深度学习</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2020/02/26/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%9D%E8%80%83/">神经网络权重初始化问题的思考</a></h2><time class="has-text-grey" datetime="2020-02-26T08:01:50.000Z">2020-02-26</time><p class="is-flex-grow-2 mt-2">问题概述
众所周知，神经网络权重不能初始化为常数，更不能全部初始化为0。依据反向传播的思想可知：每个权重的更新梯度与其之后相关的权重、神经元的输入、输出和其前一个神经元的输出有关(这里不进行具体推导)。网上很多相关文章说不能初始化为0或者常数，这个结论肯定是正确的。但是原因却不是这样会导致权重不更新了。其实完全错误，只要有误差，就会更新。
神经网络模型定义
现假设神经网络的通用形式如下：
有一个输入层，n个隐藏层，一个输出层。其中输入层和第1个隐藏层的参数权重为，第1个和第2个隐藏层之权重为，以此类推：第n个隐藏层和输出层之间参数权重为。其中输入层神经元个数为，第个隐藏层的神经元个数为，输出层的神经元个数，则。如有一个&quot;输入层+3层隐藏层+输出层&quot;构成的网络，且输入层、输出层神经元个数为5、1，隐藏层的神经..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2020/02/26/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%9D%E8%80%83/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2020/02/23/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83/cover_river.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2020/02/23/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83/"><img class="post-cover-img js-img-fadeIn" src="/2020/02/23/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83/cover_river.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><i class="tag post-item-tag">深度学习</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2020/02/23/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83/">SGD、Momentum、RMSProp、Adam等优化算法比较</a></h2><time class="has-text-grey" datetime="2020-02-23T02:16:26.000Z">2020-02-23</time><p class="is-flex-grow-2 mt-2">
公式化及解释
简单理了下深度学习中常用的集中参数优化方法，用相对统一的形式规整了下:








算法名称
公式
解释




牛顿法

为第t-1轮迭代时海森矩阵逆矩阵，即目标函数对参数二阶导数。


梯度下降(GD)

使用所有数据进行梯度下降


随机梯度下降(SGD)

使用单个数据进行梯度下降


批量梯度下降(Mini-batch GD)

使用每一小批数据进行梯度下降
(即GD与SGD的折中)


Momentum

利用累计的指数加权梯度-动量即惯性，作为每次权重更新的梯度


Nesterov Momentum


在Momentum的基础上根据下一步的新权重计算新梯度(&quot;往前多看一步&quot;)


AdaGrad

不同参数有各自的自适应学习率，即全局学习率除以累计梯度平方和


RMS..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2020/02/23/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83/">Read more</a></section></article></section><aside class="column is-hidden-mobile is-4-tablet is-3-widescreen"><div style="position: sticky; top: 50px;"><main class="aside-card-container archives-widget is-in-archive-page"><h3>Archives</h3><section><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/09/">September 2024</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/07/">July 2024</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">March 2024</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">May 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/12/">December 2022</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">May 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">October 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a><span class="archive-list-count">1</span></li></ul></section></main></div></aside></div></article><script>$claudia.fadeInImage(null, $claudia.blurBackdropImg)

window.addEventListener('resize', $claudia.throttle(function () {
    var images = document.querySelectorAll('.js-img-fadeIn')

    images.forEach($claudia.blurBackdropImg)
}, 150))</script></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com/qzq2514"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/qzq2514"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com/qzq2514"><i class="iconfont icon-ins"></i></a><!-- publish--><a title="publish" target="_blank" rel="noopener nofollow" href="https://blog.csdn.net/qzq2514"><i class="iconfont icon-rss"></i></a><!-- RSS--><!-- 知乎--><a title="zhihu" target="_blank" rel="noopener nofollow" href="//zhihu.com/people/qi-zhong-qi-62"><i class="iconfont icon-zhihu"></i></a><!-- 领英--><!-- 脸书--></section><p><span>Copyright ©</span><span> Georgeqi 2024</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script>$claudia.fadeInImage(null, $claudia.blurBackdropImg)

window.addEventListener('resize', $claudia.throttle(function () {
    var images = document.querySelectorAll('.js-img-fadeIn')

    images.forEach($claudia.blurBackdropImg)
}, 150))</script></body></html>
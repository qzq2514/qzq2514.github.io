<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>Georgeqi's Blog</title><meta name="description" content="The harder you work, the luckier you will be~"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/widget-post-list.css"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Georgeqi_Blog" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">Georgeqi's Blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><span>Archives · All</span></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><article class="post-container is-flex is-justify-content-center section container is-max-widescreen pt-4 px-2"><div class="columns is-variable is-1-tablet is-3-desktop-only is-2-widescreen is-full-width"><section class="column"><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2020/02/26/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%9D%E8%80%83/神经网络结构.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2020/02/26/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%9D%E8%80%83/"><img class="post-cover-img js-img-fadeIn" src="/2020/02/26/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%9D%E8%80%83/神经网络结构.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><i class="tag post-item-tag">深度学习</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2020/02/26/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%9D%E8%80%83/">神经网络权重初始化问题的思考</a></h2><time class="has-text-grey" datetime="2020-02-26T08:01:50.000Z">2020-02-26</time><p class="is-flex-grow-2 mt-2">问题概述
众所周知，神经网络权重不能初始化为常数，更不能全部初始化为0。依据反向传播的思想可知：每个权重的更新梯度与其之后相关的权重、神经元的输入、输出和其前一个神经元的输出有关(这里不进行具体推导)。网上很多相关文章说不能初始化为0或者常数，这个结论肯定是正确的。但是原因却不是这样会导致权重不更新了。其实完全错误，只要有误差，就会更新。
神经网络模型定义
现假设神经网络的通用形式如下： 有一个输入层，n个隐藏层，一个输出层。其中输入层和第1个隐藏层的参数权重为，第1个和第2个隐藏层之权重为，以此类推：第n个隐藏层和输出层之间参数权重为。其中输入层神经元个数为，第个隐藏层的神经元个数为，输出层的神经元个数，则。如有一个&quot;输入层+3层隐藏层+输出层&quot;构成的网络，且输入层、输出层神经元个数为5、1，隐藏层的神经..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2020/02/26/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E6%80%9D%E8%80%83/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2020/02/23/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83/cover_river.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2020/02/23/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83/"><img class="post-cover-img js-img-fadeIn" src="/2020/02/23/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83/cover_river.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><i class="tag post-item-tag">深度学习</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2020/02/23/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83/">SGD、Momentum、RMSProp、Adam等优化算法比较</a></h2><time class="has-text-grey" datetime="2020-02-23T02:16:26.000Z">2020-02-23</time><p class="is-flex-grow-2 mt-2">
公式化及解释
简单理了下深度学习中常用的集中参数优化方法，用相对统一的形式规整了下:








算法名称
公式
解释




牛顿法

为第t-1轮迭代时海森矩阵逆矩阵，即目标函数对参数二阶导数。


梯度下降(GD)

使用所有数据进行梯度下降


随机梯度下降(SGD)

使用单个数据进行梯度下降


批量梯度下降(Mini-batch GD)

使用每一小批数据进行梯度下降 (即GD与SGD的折中)


Momentum

利用累计的指数加权梯度-动量即惯性，作为每次权重更新的梯度


Nesterov Momentum
 
在Momentum的基础上根据下一步的新权重计算新梯度(&quot;往前多看一步&quot;)


AdaGrad

不同参数有各自的自适应学习率，即全局学习率除以累计梯度平方和


RMS..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2020/02/23/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2019/12/12/GAN%E5%AD%A6%E4%B9%A01-%E5%9F%BA%E7%A1%80%E7%BD%91%E7%BB%9C/network1.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2019/12/12/GAN%E5%AD%A6%E4%B9%A01-%E5%9F%BA%E7%A1%80%E7%BD%91%E7%BB%9C/"><img class="post-cover-img js-img-fadeIn" src="/2019/12/12/GAN%E5%AD%A6%E4%B9%A01-%E5%9F%BA%E7%A1%80%E7%BD%91%E7%BB%9C/network1.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/GAN"><i class="tag post-item-tag">GAN</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2019/12/12/GAN%E5%AD%A6%E4%B9%A01-%E5%9F%BA%E7%A1%80%E7%BD%91%E7%BB%9C/">GAN学习1-基础网络</a></h2><time class="has-text-grey" datetime="2019-12-12T05:24:59.000Z">2019-12-12</time><p class="is-flex-grow-2 mt-2">各类GAN算法
本文使用到的各类GAN算法，我自己完成复现，更多细节可以参考【代码】
损失函数







名称
损失函数




GAN



LSGAN



WGAN



WGAN-GP



DRAGAN



DCGAN



ACGAN



CGAN



InfoGAN



LapGAN



BEGAN



EBGAN




算法细节

GAN：生成对抗网络的开山之作,引入零和博弈的思想交替优化判别器和生成器
LSGAN：直接通过L2损失将判别器的预测结果拉近各自标签
WGAN：

判别器最后一层去掉sigmoid
生成器和判别器的loss不取log
权重剪枝:每次更新判别器的参数之后把它们的值截断到不超过一个固定常数c
不用基于动量的优化算法（包括momentum和 Adam）..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2019/12/12/GAN%E5%AD%A6%E4%B9%A01-%E5%9F%BA%E7%A1%80%E7%BD%91%E7%BB%9C/">Read more</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/%E9%9A%8F%E6%89%8B%E7%AC%94%E8%AE%B0"><i class="tag post-item-tag">随手笔记</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2018/04/12/Hexo%E5%BB%BA%E7%AB%99%E7%AC%94%E8%AE%B0/">Hexo建站笔记</a></h2><time class="has-text-grey" datetime="2018-04-12T03:25:11.000Z">2018-04-12</time><p class="is-flex-grow-2 mt-2">安装git和安装node.js就不多赘述~
安装Hexo
在指定目录下执行一下命令:
npm install -g hexo-cli
hexo init
hexo install
hexo clean
hexo generate
hexo server
然后打开显示的网址，即可看到本地部署好的网站
换主题
新下载的主题(如hexo-theme-Claudia)放在node_modules文件夹下，而不是放在站点根目录的themes文件夹下，然后在根目录的_config.yml中修改theme属性为Claudia
可能出现的问题

端口被占用: sudo lsof -i:4000 sudo kill 
上述方法不行的话就修改端口号: hexo的启动文件地址 node_modules-server.js 修改p..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2018/04/12/Hexo%E5%BB%BA%E7%AB%99%E7%AC%94%E8%AE%B0/">Read more</a></section></article><section class="paginator is-flex is-justify-content-flex-end is-flex-wrap-wrap mt-5"><a class="extend prev" rel="prev" href="/archives/"><i class="iconfont icon-prev has-text-grey"></i></a><a class="page-number" href="/archives/">1</a><span class="page-number current">2</span></section></section><aside class="column is-hidden-mobile is-4-tablet is-3-widescreen"><div style="position: sticky; top: 50px;"><main class="aside-card-container archives-widget is-in-archive-page"><h3>Archives</h3><section><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/12/">December 2022</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">May 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a><span class="archive-list-count">1</span></li></ul></section></main></div></aside></div></article><script>$claudia.fadeInImage(null, $claudia.blurBackdropImg)

window.addEventListener('resize', $claudia.throttle(function () {
    var images = document.querySelectorAll('.js-img-fadeIn')

    images.forEach($claudia.blurBackdropImg)
}, 150))</script></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com/qzq2514"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/qzq2514"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com/qzq2514"><i class="iconfont icon-ins"></i></a><!-- publish--><a title="publish" target="_blank" rel="noopener nofollow" href="https://blog.csdn.net/qzq2514"><i class="iconfont icon-rss"></i></a><!-- RSS--><!-- 知乎--><a title="zhihu" target="_blank" rel="noopener nofollow" href="//zhihu.com/people/qi-zhong-qi-62"><i class="iconfont icon-zhihu"></i></a><!-- 领英--><!-- 脸书--></section><p><span>Copyright ©</span><span> Georgeqi 2023</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script>$claudia.fadeInImage(null, $claudia.blurBackdropImg)

window.addEventListener('resize', $claudia.throttle(function () {
    var images = document.querySelectorAll('.js-img-fadeIn')

    images.forEach($claudia.blurBackdropImg)
}, 150))</script></body></html>
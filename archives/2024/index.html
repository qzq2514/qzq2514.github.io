<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>Georgeqi's Blog</title><meta name="description" content="The harder you work, the luckier you will be~"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/widget-post-list.css"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Georgeqi_Blog" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">Georgeqi's Blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><span>Archives · 2024</span></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><article class="post-container is-flex is-justify-content-center section container is-max-widescreen pt-4 px-2"><div class="columns is-variable is-1-tablet is-3-desktop-only is-2-widescreen is-full-width"><section class="column"><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2024/10/23/Diffusion%E5%AD%A6%E4%B9%A010-%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91/edit_sample.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2024/10/23/Diffusion%E5%AD%A6%E4%B9%A010-%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91/"><img class="post-cover-img js-img-fadeIn" src="/2024/10/23/Diffusion%E5%AD%A6%E4%B9%A010-%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91/edit_sample.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Diffusion"><i class="tag post-item-tag">Diffusion</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2024/10/23/Diffusion%E5%AD%A6%E4%B9%A010-%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91/">Diffusion学习10-图像编辑</a></h2><time class="has-text-grey" datetime="2024-10-23T14:20:59.000Z">2024-10-23</time><p class="is-flex-grow-2 mt-2">图像编辑是Diffusion算法中一个重要的方向，其旨在对生成或者真实的图像进行操作以得到与原图相似却某在某些方面(如背景、主体、风格等)存在不同的效果，具体应用其包含换主体、换背景、主体移动等各种方向。


图像编辑示例(原图来自InstructPix2Pix)

Prompt-to-Prompt
《Prompt-to-Prompt Image Editing with Cross-Attention Control》
【主页】【论文】【代码】


prompt2prompt算法框架


算法流程：提出一个training-free的图像编辑算法，无需任何其他数据和优化过程。作者观察到cross-attention层是控制图像空间layout与prompt中每个单词之间关系的关键元素，通过只修改promp..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2024/10/23/Diffusion%E5%AD%A6%E4%B9%A010-%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2024/09/22/Diffusion%E5%AD%A6%E4%B9%A09-%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/风格迁移示意图.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2024/09/22/Diffusion%E5%AD%A6%E4%B9%A09-%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"><img class="post-cover-img js-img-fadeIn" src="/2024/09/22/Diffusion%E5%AD%A6%E4%B9%A09-%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/风格迁移示意图.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Diffusion"><i class="tag post-item-tag">Diffusion</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2024/09/22/Diffusion%E5%AD%A6%E4%B9%A09-%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/">Diffusion学习9-风格迁移</a></h2><time class="has-text-grey" datetime="2024-09-22T14:20:59.000Z">2024-09-22</time><p class="is-flex-grow-2 mt-2">​
一个图像的风格可以看作是图像的色彩、纹理等与内容无关的信息，而风格迁移旨在保证生成的图像具有指定的风格(不管是通过prompt指定还是通过图像指定)，对于有指定内容的风格迁移，不仅要求生成结果具有指定的风格，还要保持和内容图具有相同内容(比如语义信息、layout)，风格迁移的效果可以参考下面的示意图：


风格迁移示意图,原图来自InstantStyle-Plus

Style Aligned
《Style Aligned Image Generation via Shared Attention》
【主页】【论文】【代码】


StyleAligned


算法流程：本算法提出了一个training-free的方法进行风格迁移，在生成的过程中通过共享图像之间的attention特征来保证生成风格一致..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2024/09/22/Diffusion%E5%AD%A6%E4%B9%A09-%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2024/07/15/Diffusion%E5%AD%A6%E4%B9%A08-%E7%89%B9%E5%BE%81%E4%BF%9D%E6%8C%81/特征保持示意图.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2024/07/15/Diffusion%E5%AD%A6%E4%B9%A08-%E7%89%B9%E5%BE%81%E4%BF%9D%E6%8C%81/"><img class="post-cover-img js-img-fadeIn" src="/2024/07/15/Diffusion%E5%AD%A6%E4%B9%A08-%E7%89%B9%E5%BE%81%E4%BF%9D%E6%8C%81/特征保持示意图.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Diffusion"><i class="tag post-item-tag">Diffusion</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2024/07/15/Diffusion%E5%AD%A6%E4%B9%A08-%E7%89%B9%E5%BE%81%E4%BF%9D%E6%8C%81/">Diffusion学习8-特征保持</a></h2><time class="has-text-grey" datetime="2024-07-15T14:20:59.000Z">2024-07-15</time><p class="is-flex-grow-2 mt-2">特征保持是Diffusion下一个重要的研究领域，其在给定同主体的一张或者多张图像的情况，生成该物体不同风格、位姿、朝向的图像，同时要保证原主体特征不变【比如粗粒度的类别、颜色，细粒度的纹理细节、人脸特征等】。


特征保持示意图【原效果分别来自CustomNet和PuLID】

其实在之前博客《Diffusion学习6-生成可控性》提到的DreamBooth、LoRA、Textual
Inversion等技术就是属于特征保持的算法，但是这类算法多多少少存在训练/推理耗时久、推理数据要求高等多个问题。而最近一些的特征保持算法则在训练速度、生成效果、数据要求上都得到了全方位的优化，所以这里单独开了一个篇章进行介绍。
IP-Adapter
《IP-Adapter:Text Compatible Image Pr..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2024/07/15/Diffusion%E5%AD%A6%E4%B9%A08-%E7%89%B9%E5%BE%81%E4%BF%9D%E6%8C%81/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2024/06/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%98%B4%E5%BD%B1%E7%94%9F%E6%88%90(Shadow%20Generation)/shadow_show.jpg" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2024/06/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%98%B4%E5%BD%B1%E7%94%9F%E6%88%90(Shadow%20Generation)/"><img class="post-cover-img js-img-fadeIn" src="/2024/06/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%98%B4%E5%BD%B1%E7%94%9F%E6%88%90(Shadow%20Generation)/shadow_show.jpg" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0"><i class="tag post-item-tag">论文笔记</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2024/06/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%98%B4%E5%BD%B1%E7%94%9F%E6%88%90(Shadow%20Generation)/">论文笔记-阴影生成</a></h2><time class="has-text-grey" datetime="2024-06-30T12:10:22.000Z">2024-06-30</time><p class="is-flex-grow-2 mt-2">阴影生成是属于Image
compositing的一个小分支，其旨在于将前景“粘贴复制”到背景上时，通过给前景添加合理的阴影以提升融合度，保证最终整个效果的真实感，效果如下：


阴影生成示意图

阴影生成的两大类方法主要是包括基于渲染(如借用3D模型)或者基于生成技术(如GAN或者Diffusion等)，本笔记主要是记录了后者大类的阴影生成技术。
ShadowGAN
《ShadowGAN: Shadow synthesis for virtual objects with conditional
adversarial networks》
【论文】


ShadowGAN网络结构


算法流程：是第一个提出使用深度神经网络进行阴影生成的算法，主框架基于生成对抗网络完成，包含一个生成器和两个判别器(全局+局..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2024/06/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-%E9%98%B4%E5%BD%B1%E7%94%9F%E6%88%90(Shadow%20Generation)/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2024/06/12/Pytorch-%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD/cover_river.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2024/06/12/Pytorch-%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD/"><img class="post-cover-img js-img-fadeIn" src="/2024/06/12/Pytorch-%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD/cover_river.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80"><i class="tag post-item-tag">编程语言</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2024/06/12/Pytorch-%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD/">Pytorch-数据加载</a></h2><time class="has-text-grey" datetime="2024-06-12T08:01:50.000Z">2024-06-12</time><p class="is-flex-grow-2 mt-2">
pytorch数据加载常用torch.utils.data.Dataset和torch.utils.data.DataLoader两个类进行实现，简单来说：

torch.utils.data.Dataset：完成数据的初步读取和加载，其内的每一条数据是&quot;零散&quot;的
torch.utils.data.DataLoader：对torch.utils.data.Dataset中&quot;零散&quot;的数据进行打包，同时也可以进行一些后处理操作和采样操作。

下面就通过代码的方式详细介绍上面的两个类。
torch.utils.data.Dataset
Dataset类简单来说就是完成数据的读取操作【当然也可以做一些简单操作】，pytorch中也内置了很多常用的计算机视觉的数据集【如如MNIST、CIFAR10、ImageNet..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2024/06/12/Pytorch-%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2024/03/10/Diffusion%E5%AD%A6%E4%B9%A07-DiT/DiT_Arch.PNG" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2024/03/10/Diffusion%E5%AD%A6%E4%B9%A07-DiT/"><img class="post-cover-img js-img-fadeIn" src="/2024/03/10/Diffusion%E5%AD%A6%E4%B9%A07-DiT/DiT_Arch.PNG" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Diffusion"><i class="tag post-item-tag">Diffusion</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2024/03/10/Diffusion%E5%AD%A6%E4%B9%A07-DiT/">Diffusion学习7-DiT</a></h2><time class="has-text-grey" datetime="2024-03-10T14:20:59.000Z">2024-03-10</time><p class="is-flex-grow-2 mt-2">Scalable Diffusion
Models with Transformers
【主页】【论文】【代码】


DiT Block架构

当前主流的Diffusion模型大都采用U-Net模型作为主干网络，U-Net网络的输入和输出具有同样的尺寸，自然也很在适合Diffusion中用于预测与输入noisy
latent相同尺寸的noise。但是自ViT后，Transformer结构也已经在多个视觉任务上被验证了其有效性，且相比较于CNN结构的U-Net可能还具有更好的效果。今天这篇论文则成功将transformer结构应用在Diffusion模型上，并且探究了其scalability能力，最终的实验也表明其最大的模型DiT-XL/2在ImageNet
256x256的类别条件生成上达到了SOTA（FI..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2024/03/10/Diffusion%E5%AD%A6%E4%B9%A07-DiT/">Read more</a></section></article></section><aside class="column is-hidden-mobile is-4-tablet is-3-widescreen"><div style="position: sticky; top: 50px;"><main class="aside-card-container archives-widget is-in-archive-page"><h3>Archives</h3><section><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/02/">February 2025</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/10/">October 2024</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/09/">September 2024</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/07/">July 2024</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">March 2024</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">May 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/12/">December 2022</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">May 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">October 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a><span class="archive-list-count">1</span></li></ul></section></main></div></aside></div></article><script>$claudia.fadeInImage(null, $claudia.blurBackdropImg)

window.addEventListener('resize', $claudia.throttle(function () {
    var images = document.querySelectorAll('.js-img-fadeIn')

    images.forEach($claudia.blurBackdropImg)
}, 150))</script></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com/qzq2514"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/qzq2514"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com/qzq2514"><i class="iconfont icon-ins"></i></a><!-- publish--><a title="publish" target="_blank" rel="noopener nofollow" href="https://blog.csdn.net/qzq2514"><i class="iconfont icon-rss"></i></a><!-- RSS--><!-- 知乎--><a title="zhihu" target="_blank" rel="noopener nofollow" href="//zhihu.com/people/qi-zhong-qi-62"><i class="iconfont icon-zhihu"></i></a><!-- 领英--><!-- 脸书--></section><p><span>Copyright ©</span><span> Georgeqi 2025</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script>$claudia.fadeInImage(null, $claudia.blurBackdropImg)

window.addEventListener('resize', $claudia.throttle(function () {
    var images = document.querySelectorAll('.js-img-fadeIn')

    images.forEach($claudia.blurBackdropImg)
}, 150))</script></body></html>
<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>Georgeqi's Blog</title><meta name="description" content="The harder you work, the luckier you will be~"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/widget-post-list.css"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Georgeqi_Blog" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">Georgeqi's Blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><span>Tags · Diffusion</span></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><article class="post-container is-flex is-justify-content-center section container is-max-widescreen pt-4 px-2"><div class="columns is-variable is-1-tablet is-3-desktop-only is-2-widescreen is-full-width"><section class="column"><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2025/02/04/Diffusion%E5%AD%A6%E4%B9%A011-%E5%A4%A7%E4%B8%80%E7%BB%9F%E6%A8%A1%E5%9E%8B/Omni_model_show.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2025/02/04/Diffusion%E5%AD%A6%E4%B9%A011-%E5%A4%A7%E4%B8%80%E7%BB%9F%E6%A8%A1%E5%9E%8B/"><img class="post-cover-img js-img-fadeIn" src="/2025/02/04/Diffusion%E5%AD%A6%E4%B9%A011-%E5%A4%A7%E4%B8%80%E7%BB%9F%E6%A8%A1%E5%9E%8B/Omni_model_show.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Diffusion"><i class="tag post-item-tag">Diffusion</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2025/02/04/Diffusion%E5%AD%A6%E4%B9%A011-%E5%A4%A7%E4%B8%80%E7%BB%9F%E6%A8%A1%E5%9E%8B/">Diffusion学习11-大一统模型</a></h2><time class="has-text-grey" datetime="2025-02-04T14:20:59.000Z">2025-02-04</time><p class="is-flex-grow-2 mt-2">​
Diffusion模型在图像生成领域已经具备了优越的效果，本博客之前的很多系列篇文章也分别介绍了诸如控制生成、特征保持、风格迁移、图像编辑，但是这些能力更像是插件一样依附在强大的文生图基础模型上，在我们需要某个能力的时候从”仓库“中抽一个出来然后不用再放回去。
​
联想到目前LLM下统一的生成范式，一个模型可以做所有预研相关的任务(如机器翻译、情感分析等)，那么自然而然会联想到图像生成领域能不能有这样一个模型，能同时完成如下图所示的可控生成、编辑、特征保持等各项任务？


大一统模型能做的任务(from.UniReal)

​
在这种情形下，大一统模型就应运而生，其以一个强大的backbone为基础，支持任意形式的多模态输入并同时支持以上所说的多种任务。
OmniGen
《OmniGen: Unifie..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2025/02/04/Diffusion%E5%AD%A6%E4%B9%A011-%E5%A4%A7%E4%B8%80%E7%BB%9F%E6%A8%A1%E5%9E%8B/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2024/10/23/Diffusion%E5%AD%A6%E4%B9%A010-%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91/edit_sample.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2024/10/23/Diffusion%E5%AD%A6%E4%B9%A010-%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91/"><img class="post-cover-img js-img-fadeIn" src="/2024/10/23/Diffusion%E5%AD%A6%E4%B9%A010-%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91/edit_sample.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Diffusion"><i class="tag post-item-tag">Diffusion</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2024/10/23/Diffusion%E5%AD%A6%E4%B9%A010-%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91/">Diffusion学习10-图像编辑</a></h2><time class="has-text-grey" datetime="2024-10-23T14:20:59.000Z">2024-10-23</time><p class="is-flex-grow-2 mt-2">图像编辑是Diffusion算法中一个重要的方向，其旨在对生成或者真实的图像进行操作以得到与原图相似却某在某些方面(如背景、主体、风格等)存在不同的效果，具体应用其包含换主体、换背景、主体移动等各种方向。


图像编辑示例(原图来自InstructPix2Pix)

Prompt-to-Prompt
《Prompt-to-Prompt Image Editing with Cross-Attention Control》
【主页】【论文】【代码】


prompt2prompt算法框架


算法流程：提出一个training-free的图像编辑算法，无需任何其他数据和优化过程。作者观察到cross-attention层是控制图像空间layout与prompt中每个单词之间关系的关键元素，通过只修改promp..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2024/10/23/Diffusion%E5%AD%A6%E4%B9%A010-%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2024/09/22/Diffusion%E5%AD%A6%E4%B9%A09-%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/风格迁移示意图.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2024/09/22/Diffusion%E5%AD%A6%E4%B9%A09-%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"><img class="post-cover-img js-img-fadeIn" src="/2024/09/22/Diffusion%E5%AD%A6%E4%B9%A09-%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/风格迁移示意图.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Diffusion"><i class="tag post-item-tag">Diffusion</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2024/09/22/Diffusion%E5%AD%A6%E4%B9%A09-%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/">Diffusion学习9-风格迁移</a></h2><time class="has-text-grey" datetime="2024-09-22T14:20:59.000Z">2024-09-22</time><p class="is-flex-grow-2 mt-2">​
一个图像的风格可以看作是图像的色彩、纹理等与内容无关的信息，而风格迁移旨在保证生成的图像具有指定的风格(不管是通过prompt指定还是通过图像指定)，对于有指定内容的风格迁移，不仅要求生成结果具有指定的风格，还要保持和内容图具有相同内容(比如语义信息、layout)，风格迁移的效果可以参考下面的示意图：


风格迁移示意图,原图来自InstantStyle-Plus

Style Aligned
《Style Aligned Image Generation via Shared Attention》
【主页】【论文】【代码】


StyleAligned


算法流程：本算法提出了一个training-free的方法进行风格迁移，在生成的过程中通过共享图像之间的attention特征来保证生成风格一致..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2024/09/22/Diffusion%E5%AD%A6%E4%B9%A09-%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/./%E7%89%B9%E5%BE%81%E4%BF%9D%E6%8C%81%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2024/07/15/Diffusion%E5%AD%A6%E4%B9%A08-%E7%89%B9%E5%BE%81%E4%BF%9D%E6%8C%81/"><img class="post-cover-img js-img-fadeIn" src="/./%E7%89%B9%E5%BE%81%E4%BF%9D%E6%8C%81%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Diffusion"><i class="tag post-item-tag">Diffusion</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2024/07/15/Diffusion%E5%AD%A6%E4%B9%A08-%E7%89%B9%E5%BE%81%E4%BF%9D%E6%8C%81/">Diffusion学习8-特征保持</a></h2><time class="has-text-grey" datetime="2024-07-15T14:20:59.000Z">2024-07-15</time><p class="is-flex-grow-2 mt-2">特征保持是Diffusion下一个重要的研究领域，其在给定同主体的一张或者多张图像的情况，生成该物体不同风格、位姿、朝向的图像，同时要保证原主体特征不变【比如粗粒度的类别、颜色，细粒度的纹理细节、人脸特征等】。

其实在之前博客《Diffusion学习6-生成可控性》提到的DreamBooth、LoRA、Textual Inversion等技术就是属于特征保持的算法，但是这类算法多多少少存在训练/推理耗时久、推理数据要求高等多个问题。而最近一些的特征保持算法则在训练速度、生成效果、数据要求上都得到了全方位的优化，所以这里单独开了一个篇章进行介绍。
IP-Adapter《IP-Adapter:Text Compatible Image Prompt Adapter for Text-to-Image Diff..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2024/07/15/Diffusion%E5%AD%A6%E4%B9%A08-%E7%89%B9%E5%BE%81%E4%BF%9D%E6%8C%81/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2024/03/10/Diffusion%E5%AD%A6%E4%B9%A07-DiT/DiT_Arch.PNG" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2024/03/10/Diffusion%E5%AD%A6%E4%B9%A07-DiT/"><img class="post-cover-img js-img-fadeIn" src="/2024/03/10/Diffusion%E5%AD%A6%E4%B9%A07-DiT/DiT_Arch.PNG" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Diffusion"><i class="tag post-item-tag">Diffusion</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2024/03/10/Diffusion%E5%AD%A6%E4%B9%A07-DiT/">Diffusion学习7-DiT</a></h2><time class="has-text-grey" datetime="2024-03-10T14:20:59.000Z">2024-03-10</time><p class="is-flex-grow-2 mt-2">Scalable Diffusion
Models with Transformers
【主页】【论文】【代码】


DiT Block架构

当前主流的Diffusion模型大都采用U-Net模型作为主干网络，U-Net网络的输入和输出具有同样的尺寸，自然也很在适合Diffusion中用于预测与输入noisy
latent相同尺寸的noise。但是自ViT后，Transformer结构也已经在多个视觉任务上被验证了其有效性，且相比较于CNN结构的U-Net可能还具有更好的效果。今天这篇论文则成功将transformer结构应用在Diffusion模型上，并且探究了其scalability能力，最终的实验也表明其最大的模型DiT-XL/2在ImageNet
256x256的类别条件生成上达到了SOTA（FI..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2024/03/10/Diffusion%E5%AD%A6%E4%B9%A07-DiT/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2023/06/10/Diffusion%E5%AD%A6%E4%B9%A06-%E7%94%9F%E6%88%90%E5%8F%AF%E6%8E%A7%E6%80%A7/Diffusion可控性.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2023/06/10/Diffusion%E5%AD%A6%E4%B9%A06-%E7%94%9F%E6%88%90%E5%8F%AF%E6%8E%A7%E6%80%A7/"><img class="post-cover-img js-img-fadeIn" src="/2023/06/10/Diffusion%E5%AD%A6%E4%B9%A06-%E7%94%9F%E6%88%90%E5%8F%AF%E6%8E%A7%E6%80%A7/Diffusion可控性.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Diffusion"><i class="tag post-item-tag">Diffusion</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2023/06/10/Diffusion%E5%AD%A6%E4%B9%A06-%E7%94%9F%E6%88%90%E5%8F%AF%E6%8E%A7%E6%80%A7/">Diffusion学习6-生成可控性</a></h2><time class="has-text-grey" datetime="2023-06-10T12:10:22.000Z">2023-06-10</time><p class="is-flex-grow-2 mt-2">Diffusion相比较起前辈GAN，在生成质量上已经得到了大幅度的提升，之前的文章介绍过虽然通过prompt配合上classifier-free
guidance技术能够从一定程度达到控制生成内容的目的，但是如何更加精确控制diffuison结果，例如定制化生成指定的目标对象，控制生成目标的姿态、形状、颜色等，也成为后续研究的重点，当然这些控制性生成目前也已经能够达到下图所示的非常好的效果了，本篇文章也将重点介绍些这些控制diffusion生成的大杀器。


Diffusion可控性生成效果示意效果,原图来自huggingface/lllyasviel

DreamBooth
《DreamBooth: Fine Tuning Text-to-Image Diffusion Models for
Subje..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2023/06/10/Diffusion%E5%AD%A6%E4%B9%A06-%E7%94%9F%E6%88%90%E5%8F%AF%E6%8E%A7%E6%80%A7/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2023/05/20/Diffusion%E5%AD%A6%E4%B9%A05-%E9%AB%98%E8%B4%A8%E9%87%8F%E6%96%87%E7%94%9F%E5%9B%BE%E6%A8%A1%E5%9E%8B/Diffusion_show.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2023/05/20/Diffusion%E5%AD%A6%E4%B9%A05-%E9%AB%98%E8%B4%A8%E9%87%8F%E6%96%87%E7%94%9F%E5%9B%BE%E6%A8%A1%E5%9E%8B/"><img class="post-cover-img js-img-fadeIn" src="/2023/05/20/Diffusion%E5%AD%A6%E4%B9%A05-%E9%AB%98%E8%B4%A8%E9%87%8F%E6%96%87%E7%94%9F%E5%9B%BE%E6%A8%A1%E5%9E%8B/Diffusion_show.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Diffusion"><i class="tag post-item-tag">Diffusion</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2023/05/20/Diffusion%E5%AD%A6%E4%B9%A05-%E9%AB%98%E8%B4%A8%E9%87%8F%E6%96%87%E7%94%9F%E5%9B%BE%E6%A8%A1%E5%9E%8B/">Diffusion学习5-高质量文生图模型</a></h2><time class="has-text-grey" datetime="2023-05-20T12:10:22.000Z">2023-05-20</time><p class="is-flex-grow-2 mt-2">

高质量Diffusion模型生成结果(原图来自DALLE-2)

DALLE系列
DALLE
《Zero-Shot Text-to-Image Generation》
【主页】【论文】【代码-非官方】
DALL·E的名字是为了向艺术家萨尔瓦多·达利和皮克斯的机器人WALL-E致敬
DALLE-2
《Hierarchical Text-Conditional Image Generation with CLIP
Latents》
【主页】【论文】
DALLE-3
《Improving Image Generation with Better Captions》
【主页】【论文】
GLIDE
《GLIDE: Towards Photorealistic Image Generation and Editin..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2023/05/20/Diffusion%E5%AD%A6%E4%B9%A05-%E9%AB%98%E8%B4%A8%E9%87%8F%E6%96%87%E7%94%9F%E5%9B%BE%E6%A8%A1%E5%9E%8B/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/ImprovedDDPM_figure3.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/"><img class="post-cover-img js-img-fadeIn" src="/ImprovedDDPM_figure3.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Diffusion"><i class="tag post-item-tag">Diffusion</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/">Diffusion学习4-效果和性能提升</a></h2><time class="has-text-grey" datetime="2022-12-31T14:10:22.000Z">2022-12-31</time><p class="is-flex-grow-2 mt-2">下述效果的具体代码见: qzq2514/Diffusion
效果提升这里简单实验和讨论了在训练DDPM过程中使用的损失函数(L1或L2损失)，并且实验了Improved DDPM 中提到的Cosine Beta Schedule带来的效果提升。
Cosine Beta Schedule首先回顾下DDPM在前向过程中的一个重要公式-利用重采样技巧从直接得到，即Diffusion学习2-理论推导中的公式(3):而且在原DDPM论文中使用的线性Beta采样，即:
torch.linspace(start=0.0001, end=0.02, steps=100)

Improved DDPM则进一步分析了这种Beta设计的缺点，取出论文中的图3和图5:



Linear和Cosine的Beta方式对于加噪的影响
..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2022/12/25/Diffusion%E5%AD%A6%E4%B9%A03-%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5/20221228191848_StyleGAN_face_size128_225_progress.jpg" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2022/12/25/Diffusion%E5%AD%A6%E4%B9%A03-%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5/"><img class="post-cover-img js-img-fadeIn" src="/2022/12/25/Diffusion%E5%AD%A6%E4%B9%A03-%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5/20221228191848_StyleGAN_face_size128_225_progress.jpg" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Diffusion"><i class="tag post-item-tag">Diffusion</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2022/12/25/Diffusion%E5%AD%A6%E4%B9%A03-%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5/">Diffusion学习3-代码实践</a></h2><time class="has-text-grey" datetime="2022-12-25T13:27:59.000Z">2022-12-25</time><p class="is-flex-grow-2 mt-2">先放一张在人脸数据集上训练好后去噪的可视化过程


DDPM可视化去噪

各数据集参数配置及效果
训练:
CUDA_VISIBLE_DEVICES=0 python train_solver.py --data_name &quot;Flower102&quot;
在config.yaml中各个数据集使用默认的Training
Setting，每个数据集特有的配置见config.yaml下的Train_Data.
生成效果
生成效果如下:









数据集
去噪过程可视化
最终去噪效果
插值




Mnist





Fashion_Mnist





Cifar10





Flower102





StyleGAN2人脸






上述训练数据集和已经训练好的模型放在这里.

去噪过程可视化中，如果在..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2022/12/25/Diffusion%E5%AD%A6%E4%B9%A03-%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="/2022/12/20/Diffusion%E5%AD%A6%E4%B9%A02-%E7%90%86%E8%AE%BA%E6%8E%A8%E5%AF%BC/各种生成模型总览.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2022/12/20/Diffusion%E5%AD%A6%E4%B9%A02-%E7%90%86%E8%AE%BA%E6%8E%A8%E5%AF%BC/"><img class="post-cover-img js-img-fadeIn" src="/2022/12/20/Diffusion%E5%AD%A6%E4%B9%A02-%E7%90%86%E8%AE%BA%E6%8E%A8%E5%AF%BC/各种生成模型总览.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Diffusion"><i class="tag post-item-tag">Diffusion</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2022/12/20/Diffusion%E5%AD%A6%E4%B9%A02-%E7%90%86%E8%AE%BA%E6%8E%A8%E5%AF%BC/">Diffusion学习2-理论推导</a></h2><time class="has-text-grey" datetime="2022-12-20T13:27:59.000Z">2022-12-20</time><p class="is-flex-grow-2 mt-2">概述


生成模型总览(原图来自lilianweng.blog)

Diffusion属于生成模型的一种，相比较于GAN等其他生成模型，Diffusion模型最大的不同之处就在于其latent
code是和原输入图相同尺寸的。Diffusion模型其实也可以看成是一个隐变量模型，并且与VAE,GAN的单隐变量不同，其可以看成存在多个隐变量(即加噪过程中的每个加噪结果都可以看成一个隐变量)。Diffusion模型总体包括前向加噪和逆向去噪两个过程:

前向过程-加噪扩散:对给定的真实图像不断添加高斯噪声，经过中间状态最终变成纯高斯噪声
逆向过程-去噪生成:从完全的纯噪声不断去噪，经过中间状态最终变成其对应的真实图像

上面两个过程示意图可以表示如下:


Diffusion的前向和逆向过程(原图来自Ho et..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2022/12/20/Diffusion%E5%AD%A6%E4%B9%A02-%E7%90%86%E8%AE%BA%E6%8E%A8%E5%AF%BC/">Read more</a></section></article><section class="paginator is-flex is-justify-content-flex-end is-flex-wrap-wrap mt-5"><span class="page-number current">1</span><a class="page-number" href="/tags/Diffusion/page/2/">2</a><a class="extend next" rel="next" href="/tags/Diffusion/page/2/"><i class="iconfont icon-next has-text-grey"></i></a></section></section><aside class="column is-hidden-mobile is-4-tablet is-3-widescreen"><div style="position: sticky; top: 50px;"><main class="aside-card-container tag-widget is-in-tag-page"><h3>Tags</h3><section><a href="/tags/Diffusion"><span class="tag post-item-tag" style="margin-bottom: 5px;">Diffusion</span></a><a href="/tags/GAN"><span class="tag post-item-tag" style="margin-bottom: 5px;">GAN</span></a><a href="/tags/%E9%9A%8F%E6%89%8B%E7%AC%94%E8%AE%B0"><span class="tag post-item-tag" style="margin-bottom: 5px;">随手笔记</span></a><a href="/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80"><span class="tag post-item-tag" style="margin-bottom: 5px;">编程语言</span></a><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="tag post-item-tag" style="margin-bottom: 5px;">深度学习</span></a><a href="/tags/%E8%BD%A6%E7%89%8C%E8%AF%86%E5%88%AB"><span class="tag post-item-tag" style="margin-bottom: 5px;">车牌识别</span></a><a href="/tags/%E7%AE%97%E6%B3%95%E5%88%B7%E9%A2%98"><span class="tag post-item-tag" style="margin-bottom: 5px;">算法刷题</span></a><a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0"><span class="tag post-item-tag" style="margin-bottom: 5px;">论文笔记</span></a><a href="/tags/Mesh%E7%BA%B9%E7%90%86%E7%94%9F%E6%88%90"><span class="tag post-item-tag" style="margin-bottom: 5px;">Mesh纹理生成</span></a></section></main></div></aside></div></article><script>$claudia.fadeInImage(null, $claudia.blurBackdropImg)

window.addEventListener('resize', $claudia.throttle(function () {
    var images = document.querySelectorAll('.js-img-fadeIn')

    images.forEach($claudia.blurBackdropImg)
}, 150))</script></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com/qzq2514"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/qzq2514"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com/qzq2514"><i class="iconfont icon-ins"></i></a><!-- publish--><a title="publish" target="_blank" rel="noopener nofollow" href="https://blog.csdn.net/qzq2514"><i class="iconfont icon-rss"></i></a><!-- RSS--><!-- 知乎--><a title="zhihu" target="_blank" rel="noopener nofollow" href="//zhihu.com/people/qi-zhong-qi-62"><i class="iconfont icon-zhihu"></i></a><!-- 领英--><!-- 脸书--></section><p><span>Copyright ©</span><span> Georgeqi 2025</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script>$claudia.fadeInImage(null, $claudia.blurBackdropImg)

window.addEventListener('resize', $claudia.throttle(function () {
    var images = document.querySelectorAll('.js-img-fadeIn')

    images.forEach($claudia.blurBackdropImg)
}, 150))</script></body></html>
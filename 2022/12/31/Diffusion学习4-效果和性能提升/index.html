<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>Diffusion学习4-效果和性能提升</title><meta name="description" content="The harder you work, the luckier you will be~"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><link rel="stylesheet" href="/style/common/jquery.fancybox.min.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="效果提升
这里简单实验和讨论了在训练DDPM过程中使用的损失函数(L1或L2损失)，并且实验了Improved DDPM 中提到的Cosine Beta Schedule带来的效果提升。
Cosine Beta Schedule
待补充~
效果提升实践

单通道较简单的数据集(如Mnist, Fashion_Mnist等)可以直接使用Linear的Beta采样，与Cosine采样无大区别
3通道相对复杂的数据集(如Cifar10, Flower102, StyleGAN_Face等)的Beta采样最好使用Improved DDPM中提出的Cosine schedule,不然会导致最终生成的图片偏白。
L1损失和L2损失对比，L2损失下生成效果会更加&quot;尖锐&quot;有时候会稍显乱+脏，而L1损失则显得更加平滑。

上.."><meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="Georgeqi_Blog" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">Georgeqi's Blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Diffusion学习4-效果和性能提升</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%88%E6%9E%9C%E6%8F%90%E5%8D%87"><span class="toc-text">效果提升</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#cosine-beta-schedule"><span class="toc-text">Cosine Beta Schedule</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%88%E6%9E%9C%E6%8F%90%E5%8D%87%E5%AE%9E%E8%B7%B5"><span class="toc-text">效果提升实践</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%87%87%E6%A0%B7%E5%8A%A0%E9%80%9F"><span class="toc-text">采样加速</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#ddim%E5%85%AC%E5%BC%8F%E6%8E%A8%E7%90%86"><span class="toc-text">DDIM公式推理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A0%E9%80%9F%E9%87%87%E6%A0%B7%E5%AE%9E%E8%B7%B5"><span class="toc-text">加速采样实践</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B6%E4%BB%96tricks"><span class="toc-text">其他Tricks</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AEcode"><span class="toc-text">参考文献&#x2F;Code</span></a></li></ol></div><div class="column is-9"><header class="my-4"><a href="/tags/Diffusion"><i class="tag post-item-tag">Diffusion</i></a></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">Diffusion学习4-效果和性能提升</h1><time class="has-text-grey" datetime="2022-12-31T14:10:22.000Z">2022-12-31</time><article class="mt-2 post-content"><h1 id="效果提升">效果提升</h1>
<p>这里简单实验和讨论了在训练DDPM过程中使用的损失函数(L1或L2损失)，并且实验了<a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v139/nichol21a/nichol21a.pdf">Improved DDPM</a> 中提到的Cosine Beta Schedule带来的效果提升。</p>
<h2 id="cosine-beta-schedule">Cosine Beta Schedule</h2>
<p>待补充~</p>
<h2 id="效果提升实践">效果提升实践</h2>
<ul>
<li>单通道较简单的数据集(如Mnist, Fashion_Mnist等)可以直接使用Linear的Beta采样，与Cosine采样无大区别</li>
<li>3通道相对复杂的数据集(如Cifar10, Flower102, StyleGAN_Face等)的Beta采样最好使用<a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v139/nichol21a/nichol21a.pdf">Improved DDPM</a>中提出的Cosine schedule,不然会导致最终生成的图片偏白。</li>
<li>L1损失和L2损失对比，L2损失下生成效果会更加"尖锐"有时候会稍显乱+脏，而L1损失则显得更加平滑。</li>
</ul>
<p>上述提到的L1/L2损失、Linear Beta Schedule/Cosine Beta Schedule的效果对比如下:</p>
<table>
<thead>
<tr class="header">
<th>配置</th>
<th>L1 loss、Cosine Beta</th>
<th>L2 loss、Cosine Beta</th>
<th>L1 loss、Linear Beta</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Cifar10</td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/cifar10_epoch27.jpg"></td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/cifar10_L2loss_epoch27.jpg"></td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/cifar10_linearBeta_epoch27.jpg"></td>
</tr>
<tr class="even">
<td>Flower102</td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/Flower102_epoch_520.jpg"></td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/Flower102_L2loss_epoch_520.jpg"></td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/FLower102_linearBeta_epoch_520.jpg"></td>
</tr>
</tbody>
</table>
<h1 id="采样加速">采样加速</h1>
<p>推理加速主要使用<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.02502">DDIM</a> 中的算法</p>
<h2 id="ddim公式推理">DDIM公式推理</h2>
<p>待补充~</p>
<h2 id="加速采样实践">加速采样实践</h2>
<table>
<thead>
<tr class="header">
<th>配置</th>
<th>50步</th>
<th>100步</th>
<th>200步</th>
<th>500步</th>
<th>800步</th>
<th>1000步</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>DDPM</td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/20221231153752_StyleGAN_face_size128_DDPM_50_0p0_final.jpg"></td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/20221231153902_StyleGAN_face_size128_DDPM_100_0p0_final.jpg"></td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/20221231153924_StyleGAN_face_size128_DDPM_200_0p0_final.jpg"></td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/20221231154000_StyleGAN_face_size128_DDPM_500_0p0_final.jpg"></td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/20221231154126_StyleGAN_face_size128_DDPM_800_0p0_final.jpg"></td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/20221231154212_StyleGAN_face_size128_DDPM_1000_0p0_final.jpg"></td>
</tr>
<tr class="even">
<td>DDIM(eta=1)</td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/20221231154809_StyleGAN_face_size128_DDIM_50_1p0_final.jpg"></td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/20221231154843_StyleGAN_face_size128_DDIM_100_1p0_final.jpg"></td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/20221231154953_StyleGAN_face_size128_DDIM_200_1p0_final.jpg"></td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/20221231155049_StyleGAN_face_size128_DDIM_500_1p0_final.jpg"></td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/20221231155129_StyleGAN_face_size128_DDIM_800_1p0_final.jpg"></td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/20221231155212_StyleGAN_face_size128_DDIM_1000_1p0_final.jpg"></td>
</tr>
<tr class="odd">
<td>DDIM(eta=0)</td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/20221231154412_StyleGAN_face_size128_DDIM_50_0p0_final.jpg"></td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/20221231154734_StyleGAN_face_size128_DDIM_100_0p0_final.jpg"></td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/20221231154454_StyleGAN_face_size128_DDIM_200_0p0_final.jpg"></td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/20221231154518_StyleGAN_face_size128_DDIM_500_0p0_final.jpg"></td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/20221231154641_StyleGAN_face_size128_DDIM_800_0p0_final.jpg"></td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/20221231154551_StyleGAN_face_size128_DDIM_1000_0p0_final.jpg"></td>
</tr>
</tbody>
</table>
<p>上面的DDPM在采样步数小于训练步数(1000)的时候，使用的是<a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v139/nichol21a/nichol21a.pdf">Improved DDPM</a> 中的间隔步长采样。可以看到上面的结果：</p>
<ul>
<li>常规的DDPM采样结果在步数比较小时候基本仍然为纯噪声，虽然在步数逐渐增大(如500和800步)后会逐渐出现生成主体，但是仍然有一层噪声，直到采样步数等于训练时候的去噪步数(1000步)才能生成比较好完全无噪声的图像。</li>
<li>使用DDIM算法时候即便在部署很小(50步)时，也已经具有了比较好的生成效果，生成速度能提升至少20倍。</li>
<li>从当前实验结果看下来，在DDIM采样中eta=0和eta=1好像并无太大的效果差异。</li>
</ul>
<h1 id="其他tricks">其他Tricks</h1>
<p>此外又使用<a target="_blank" rel="noopener" href="https://github.com/lucidrains/denoising-diffusion-pytorch">lucidrains</a> 的代码验证了一下其中比较重要配置(如EMA、P2 loss、Clip_denoised)的必要性，对比效果如下:</p>
<table>
<thead>
<tr class="header">
<th>配置</th>
<th>EMA✅ P2 loss✅ Clip denoised✅</th>
<th>EMA❌ P2 loss✅ Clip denoised✅</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>效果(26K steps)</td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/StyleGANFace_128-sample-130.png"></td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/StyleGANFace_128_noEMA-sample-130.png"></td>
</tr>
<tr class="even">
<td><strong>配置</strong></td>
<td><strong>EMA✅ P2 loss❌ Clip denoised✅</strong></td>
<td><strong>EMA✅ P2 loss✅ Clip denoised❌</strong></td>
</tr>
<tr class="odd">
<td>效果(26K steps)</td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/StyleGANFace_128_noP2weight-sample-130.png"></td>
<td><img src="/2022/12/31/Diffusion%E5%AD%A6%E4%B9%A04-%E6%95%88%E6%9E%9C%E5%92%8C%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87/StyleGANFace_128_noClipDenoised-sample-130.png"></td>
</tr>
</tbody>
</table>
<p>上面看来无明显的大差别，所以在本仓库中未使用EMA和P2 Loss，但是用了Clip denoised。</p>
<p>其他重要的配置则使用<a target="_blank" rel="noopener" href="https://github.com/lucidrains/denoising-diffusion-pytorch">lucidrains</a>原代码仓库中默认的:</p>
<pre><code>loss_type='l1'     beta_schedule='cosine'     objective='pred_noise'   self_condition=False</code></pre>
<h1 id="参考文献code">参考文献/Code</h1>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2006.11239.pdf">DDPM</a></p></li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.02502">DDIM</a></p></li>
<li><p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v139/nichol21a/nichol21a.pdf">Improved DDPM</a><br>
</p></li>
<li><p><a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are Diffusion Models</a><br>
</p></li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/lucidrains/denoising-diffusion-pytorch">lucidrains/denoising-diffusion-pytorch</a><br>
</p></li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/zoubohao/DenoisingDiffusionProbabilityModel-ddpm-">zoubohao/DenoisingDiffusionProbabilityModel-ddpm</a><br>
</p></li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/LianShuaiLong/CV_Applications">LianShuaiLong/CV_Applications</a><br>
</p></li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/yiyixuxu/denoising-diffusion-flax">yiyixuxu/denoising-diffusion-flax</a></p></li>
</ul>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><em></em><a class="button is-default" href="/2022/12/25/Diffusion%E5%AD%A6%E4%B9%A03-%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5/" title="Diffusion学习3-代码实践"><span class="has-text-weight-semibold">Next: Diffusion学习3-代码实践</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="qzq2514/qzq2514.github.io" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com/qzq2514"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/qzq2514"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com/qzq2514"><i class="iconfont icon-ins"></i></a><!-- publish--><a title="publish" target="_blank" rel="noopener nofollow" href="https://blog.csdn.net/qzq2514"><i class="iconfont icon-rss"></i></a><!-- RSS--><!-- 知乎--><a title="zhihu" target="_blank" rel="noopener nofollow" href="//zhihu.com/people/qi-zhong-qi-62"><i class="iconfont icon-zhihu"></i></a><!-- 领英--><!-- 脸书--></section><p><span>Copyright ©</span><span> Georgeqi 2022</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/jquery-3.6.1.min.js"></script><script src="/js/jquery-fancybox.min.js"></script><script src="/js/img_zoom.js"></script><script src="/js/post.js"></script></body></html>